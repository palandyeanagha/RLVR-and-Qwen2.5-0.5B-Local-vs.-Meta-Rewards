{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful\n"
     ]
    }
   ],
   "source": [
    "# baseline_evaluation_v1.ipynb\n",
    "\n",
    "# Cell 1: Setup\n",
    "from main import GSM8KRLTrainer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen/Qwen2.5-0.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train split from data/train.jsonl...\n",
      "Loaded train split: 6352 examples\n",
      "Loading valid split from data/valid.jsonl...\n",
      "Loaded valid split: 1121 examples\n",
      "Loading test split from data/test.jsonl...\n",
      "Loaded test split: 1319 examples\n",
      "Reward type: meta_only\n",
      "Alpha (local): 0.3, Beta (meta): 0.7\n",
      "Train: 6352 examples\n",
      "Valid: 1121 examples\n",
      "Test: 1319 examples\n",
      "TOTAL: 8792 examples\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Initialize\n",
    "trainer = GSM8KRLTrainer(\n",
    "    model_name=\"Qwen/Qwen2.5-0.5B\",\n",
    "    reward_type='meta_only',\n",
    "    data_dir='data'\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(trainer.train_dataset)} examples\")\n",
    "print(f\"Valid: {len(trainer.valid_dataset)} examples\")\n",
    "print(f\"Test: {len(trainer.test_dataset)} examples\")\n",
    "print(f\"TOTAL: {len(trainer.train_dataset) + len(trainer.valid_dataset) + len(trainer.test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will append to existing baseline_results_full.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Setup output file\n",
    "output_file = 'baseline_results_full.jsonl'\n",
    "summary_file = 'baseline_summary.json'\n",
    "\n",
    "# Create or clear output file\n",
    "if not os.path.exists(output_file):\n",
    "    open(output_file, 'w').close()\n",
    "    print(f\"Created {output_file}\")\n",
    "else:\n",
    "    print(f\"Will append to existing {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating TRAIN: 6000 to 6352\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 6000-6352: 100%|██████████| 352/352 [1:19:39<00:00, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch accuracy: 156/352 = 44.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Evaluate Train Set (run multiple times for batches)\n",
    "dataset = trainer.train_dataset\n",
    "split_name = 'train'\n",
    "start_idx = 6000  # CHANGE THIS for each batch: 0, 1000, 2000, etc.\n",
    "batch_size = 1000 #10 #1000\n",
    "\n",
    "end_idx = min(start_idx + batch_size, len(dataset))\n",
    "eval_data = dataset.data[start_idx:end_idx]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Evaluating {split_name.upper()}: {start_idx} to {end_idx}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for item in tqdm(eval_data, desc=f\"{split_name} {start_idx}-{end_idx}\"):\n",
    "    question = item['question']\n",
    "    gt_answer = dataset.extract_final_answer(item['answer'])\n",
    "    \n",
    "    # Generate\n",
    "    response = trainer.generate_response(question)\n",
    "    model_answer = trainer.reward_computer.extract_model_answer(response)\n",
    "    \n",
    "    # Check\n",
    "    is_correct = (model_answer is not None and abs(model_answer - gt_answer) < 1e-6)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    # Append to file immediately\n",
    "    result = {\n",
    "        'split': split_name,\n",
    "        'question': question,\n",
    "        'ground_truth_answer': gt_answer,\n",
    "        'model_response': response,\n",
    "        'extracted_answer': model_answer,\n",
    "        'correct': 1 if is_correct else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "print(f\"\\nBatch accuracy: {correct}/{total} = {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating VALID: 1000 to 1121\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid 1000-1121: 100%|██████████| 121/121 [27:27<00:00, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch accuracy: 67/121 = 55.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluate Valid Set (same pattern)\n",
    "dataset = trainer.valid_dataset\n",
    "split_name = 'valid'\n",
    "start_idx = 1000  # CHANGE THIS for batches\n",
    "batch_size = 1000\n",
    "\n",
    "end_idx = min(start_idx + batch_size, len(dataset))\n",
    "eval_data = dataset.data[start_idx:end_idx]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Evaluating {split_name.upper()}: {start_idx} to {end_idx}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for item in tqdm(eval_data, desc=f\"{split_name} {start_idx}-{end_idx}\"):\n",
    "    question = item['question']\n",
    "    gt_answer = dataset.extract_final_answer(item['answer'])\n",
    "    \n",
    "    response = trainer.generate_response(question)\n",
    "    model_answer = trainer.reward_computer.extract_model_answer(response)\n",
    "    \n",
    "    is_correct = (model_answer is not None and abs(model_answer - gt_answer) < 1e-6)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    result = {\n",
    "        'split': split_name,\n",
    "        'question': question,\n",
    "        'ground_truth_answer': gt_answer,\n",
    "        'model_response': response,\n",
    "        'extracted_answer': model_answer,\n",
    "        'correct': 1 if is_correct else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "print(f\"\\nBatch accuracy: {correct}/{total} = {correct/total:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Evaluating TEST: 1000 to 1319\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test 1000-1319: 100%|██████████| 319/319 [1:10:46<00:00, 13.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch accuracy: 125/319 = 39.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Evaluate Test Set (same pattern)\n",
    "dataset = trainer.test_dataset\n",
    "split_name = 'test'\n",
    "start_idx = 1000  # CHANGE THIS for batches\n",
    "batch_size = 1000\n",
    "\n",
    "end_idx = min(start_idx + batch_size, len(dataset))\n",
    "eval_data = dataset.data[start_idx:end_idx]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Evaluating {split_name.upper()}: {start_idx} to {end_idx}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for item in tqdm(eval_data, desc=f\"{split_name} {start_idx}-{end_idx}\"):\n",
    "    question = item['question']\n",
    "    gt_answer = dataset.extract_final_answer(item['answer'])\n",
    "    \n",
    "    response = trainer.generate_response(question)\n",
    "    model_answer = trainer.reward_computer.extract_model_answer(response)\n",
    "    \n",
    "    is_correct = (model_answer is not None and abs(model_answer - gt_answer) < 1e-6)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    \n",
    "    result = {\n",
    "        'split': split_name,\n",
    "        'question': question,\n",
    "        'ground_truth_answer': gt_answer,\n",
    "        'model_response': response,\n",
    "        'extracted_answer': model_answer,\n",
    "        'correct': 1 if is_correct else 0\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'a') as f:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "print(f\"\\nBatch accuracy: {correct}/{total} = {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CALCULATING OVERALL ACCURACY FROM ALL RESULTS\n",
      "================================================================================\n",
      "\n",
      "Train:  3010/6352 = 47.39%\n",
      "Valid:  537/1121 = 47.90%\n",
      "Test:   437/1319 = 33.13%\n",
      "\n",
      "================================================================================\n",
      "OVERALL ACCURACY: 3984/8792 = 45.31%\n",
      "================================================================================\n",
      "\n",
      "Summary saved to baseline_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Calculate FULL dataset accuracy\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CALCULATING OVERALL ACCURACY FROM ALL RESULTS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "split_stats = {'train': {'correct': 0, 'total': 0},\n",
    "               'valid': {'correct': 0, 'total': 0},\n",
    "               'test': {'correct': 0, 'total': 0}}\n",
    "\n",
    "with open(output_file, 'r') as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line)\n",
    "        split = result['split']\n",
    "        \n",
    "        split_stats[split]['total'] += 1\n",
    "        split_stats[split]['correct'] += result['correct']\n",
    "        \n",
    "        total_correct += result['correct']\n",
    "        total_samples += 1\n",
    "\n",
    "# Overall accuracy\n",
    "overall_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "print(f\"Train:  {split_stats['train']['correct']}/{split_stats['train']['total']} = {split_stats['train']['correct']/split_stats['train']['total']:.2%}\")\n",
    "print(f\"Valid:  {split_stats['valid']['correct']}/{split_stats['valid']['total']} = {split_stats['valid']['correct']/split_stats['valid']['total']:.2%}\")\n",
    "print(f\"Test:   {split_stats['test']['correct']}/{split_stats['test']['total']} = {split_stats['test']['correct']/split_stats['test']['total']:.2%}\")\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL ACCURACY: {total_correct}/{total_samples} = {overall_accuracy:.2%}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'overall': {\n",
    "        'correct': total_correct,\n",
    "        'total': total_samples,\n",
    "        'accuracy': overall_accuracy\n",
    "    },\n",
    "    'by_split': {\n",
    "        'train': {\n",
    "            'correct': split_stats['train']['correct'],\n",
    "            'total': split_stats['train']['total'],\n",
    "            'accuracy': split_stats['train']['correct']/split_stats['train']['total'] if split_stats['train']['total'] > 0 else 0.0\n",
    "        },\n",
    "        'valid': {\n",
    "            'correct': split_stats['valid']['correct'],\n",
    "            'total': split_stats['valid']['total'],\n",
    "            'accuracy': split_stats['valid']['correct']/split_stats['valid']['total'] if split_stats['valid']['total'] > 0 else 0.0\n",
    "        },\n",
    "        'test': {\n",
    "            'correct': split_stats['test']['correct'],\n",
    "            'total': split_stats['test']['total'],\n",
    "            'accuracy': split_stats['test']['correct']/split_stats['test']['total'] if split_stats['test']['total'] > 0 else 0.0\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(summary_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Summary saved to {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
